{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "image_classification_with_CNN+Attention Model",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "environment": {
      "name": "tf2-gpu.2-4.m61",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m61"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lq-gTE-uBtXC"
      },
      "source": [
        "# Image classification with CNN+Attention Hybrid Model\n",
        "\n",
        "**Author:** [RIYAJ ATAR]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3E8ZXYWufDFV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2a69f85-9b69-40a5-c3fc-01d9fbf69b27"
      },
      "source": [
        "!pip install -U tensorflow-addons"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.13.0-cp37-cp37m-manylinux2010_x86_64.whl (679 kB)\n",
            "\u001b[?25l\r\u001b[K     |▌                               | 10 kB 34.5 MB/s eta 0:00:01\r\u001b[K     |█                               | 20 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 30 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |██                              | 40 kB 16.2 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 51 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |███                             | 61 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 71 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 81 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 92 kB 9.4 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 102 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 112 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 122 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 133 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 143 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 153 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 163 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 174 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 184 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 194 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 204 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 215 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 225 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 235 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 245 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 256 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 266 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 276 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 286 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 296 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 307 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 317 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 327 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 337 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 348 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 358 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 368 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 378 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 389 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 399 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 409 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 419 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 430 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 440 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 450 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 460 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 471 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 481 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 491 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 501 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 512 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 522 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 532 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 542 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 552 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 563 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 573 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 583 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 593 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 604 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 614 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 624 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 634 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 645 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 655 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 665 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 675 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 679 kB 7.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.13.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5P2vw2QBtXF"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUrSOCUNBtXF"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aCipVZABtXG"
      },
      "source": [
        "## Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nANkPcNNBtXG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "629471fd-85e4-436e-cedf-87be9ee9d42c"
      },
      "source": [
        "num_classes = 100\n",
        "input_shape = (32, 32, 3)\n",
        "image_size = 72\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
        "\n",
        "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
        "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "169009152/169001437 [==============================] - 4s 0us/step\n",
            "x_train shape: (50000, 32, 32, 3) - y_train shape: (50000, 1)\n",
            "x_test shape: (10000, 32, 32, 3) - y_test shape: (10000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXHtxwnF716C"
      },
      "source": [
        "## Attention applied on Feature Maps of CNN output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hRTNRuZ7nBT"
      },
      "source": [
        "\n",
        "def attention_weight(f,num_heads = 8):\n",
        "\n",
        "  x1 = layers.Reshape([f.shape[1]*f.shape[2],f.shape[3]])(f)\n",
        "\n",
        "  attention_output = layers.MultiHeadAttention(\n",
        "      num_heads=num_heads, key_dim=f.shape[3], dropout=0.1\n",
        "  )(x1, x1)\n",
        "\n",
        "  x1 = layers.Reshape([f.shape[1],f.shape[2],f.shape[3]])(x1)\n",
        "\n",
        "  f = layers.Multiply()([f,x1])\n",
        "\n",
        "  return f"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yu3BSAVsBtXG"
      },
      "source": [
        "## Configure the hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVGzC28IBtXG"
      },
      "source": [
        "learning_rate = 0.001\n",
        "weight_decay = 0.0001\n",
        "batch_size = 256\n",
        "num_epochs = 100\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlC29thCBtXH"
      },
      "source": [
        "## Use data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXHfGvny7uf7"
      },
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.experimental.preprocessing.Normalization(),\n",
        "        layers.experimental.preprocessing.Resizing(image_size, image_size),\n",
        "        layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
        "        layers.experimental.preprocessing.RandomRotation(factor=0.02),\n",
        "        layers.experimental.preprocessing.RandomZoom(\n",
        "            height_factor=0.2, width_factor=0.2\n",
        "        ),\n",
        "    ],\n",
        "    name=\"data_augmentation\",\n",
        ")\n",
        "# Compute the mean and the variance of the training data for normalization.\n",
        "data_augmentation.layers[0].adapt(x_train)\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViGtsipi8h03"
      },
      "source": [
        "## Define keras model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbVbAjip8lg5",
        "outputId": "060d4257-66a6-46ac-bd17-a89b653ae9d6"
      },
      "source": [
        "\n",
        "\n",
        "def model_cifar100():\n",
        "\n",
        "  x = layers.Input(shape=(32,32,3))\n",
        "\n",
        "  inputs = data_augmentation(x)\n",
        "\n",
        "\n",
        "  # l = AveragePooling2D\n",
        "\n",
        "  f = layers.Conv2D(16,3,1,'same',activation = 'relu')(inputs)\n",
        "  f = layers.Conv2D(16,3,2,'same',activation = 'relu')(f) \n",
        "  f = layers.BatchNormalization()(f)\n",
        "  # p1 = l(pool_size=2)(x)\n",
        "  # f = Concatenate(axis=3)([f,p1])\n",
        "\n",
        "  f = layers.Conv2D(32,3,1,'same',activation = 'relu')(f)\n",
        "  f = layers.Conv2D(32,3,2,'same',activation = 'relu')(f) \n",
        "  f = layers.BatchNormalization()(f)\n",
        "\n",
        "  # p2 =  l(pool_size=4)(x)\n",
        "  # f = Concatenate(axis=3)([f,p2])\n",
        "\n",
        "\n",
        "  f = layers.Conv2D(64,3,1,'same',activation = 'relu')(f)\n",
        "  f = layers.Conv2D(64,3,2,'same',activation = 'relu')(f) \n",
        "  f = layers.BatchNormalization()(f)\n",
        "\n",
        "  # p3 =  l(pool_size=8)(x)\n",
        "  # f = Concatenate(axis=3)([f,p3])\n",
        "  f = attention_weight(f,num_heads=8)\n",
        "\n",
        "  f = layers.Conv2D(128,3,1,'same',activation = 'relu')(f)\n",
        "  f = layers.Conv2D(128,3,2,'same',activation = 'relu')(f) \n",
        "  f = layers.BatchNormalization()(f)\n",
        "\n",
        "  # p4 =  l(pool_size=16)(x)\n",
        "  # f = Concatenate(axis=3)([f,p4])\n",
        "  f = attention_weight(f,num_heads=16)\n",
        "\n",
        "  representation = layers.Flatten()(f)\n",
        "  representation = layers.Dropout(0.5)(representation)\n",
        "\n",
        "\n",
        "  logits = layers.Dense(100)(representation)\n",
        "  model = keras.Model(inputs = x,outputs = logits)\n",
        "\n",
        "  return model\n",
        "\n",
        "model = model_cifar100()\n",
        "model.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "data_augmentation (Sequential)  (None, 72, 72, 3)    7           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 72, 72, 16)   448         data_augmentation[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 36, 36, 16)   2320        conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 36, 36, 16)   64          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 36, 36, 32)   4640        batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 18, 18, 32)   9248        conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 18, 18, 32)   128         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 18, 18, 64)   18496       batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 9, 9, 64)     36928       conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 9, 9, 64)     256         conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "reshape (Reshape)               (None, 81, 64)       0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "reshape_1 (Reshape)             (None, 9, 9, 64)     0           reshape[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "multiply (Multiply)             (None, 9, 9, 64)     0           batch_normalization_2[0][0]      \n",
            "                                                                 reshape_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 9, 9, 128)    73856       multiply[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 5, 5, 128)    147584      conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 5, 5, 128)    512         conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "reshape_2 (Reshape)             (None, 25, 128)      0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "reshape_3 (Reshape)             (None, 5, 5, 128)    0           reshape_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "multiply_1 (Multiply)           (None, 5, 5, 128)    0           batch_normalization_3[0][0]      \n",
            "                                                                 reshape_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 3200)         0           multiply_1[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 3200)         0           flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 100)          320100      dropout[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 614,587\n",
            "Trainable params: 614,100\n",
            "Non-trainable params: 487\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRyyX5DKBtXL"
      },
      "source": [
        "## Compile, train, and evaluate the mode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWMJvgxEBtXL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59ff7a10-fa98-4bbd-9de9-703ca6e409e3"
      },
      "source": [
        "\n",
        "def run_experiment(model):\n",
        "    optimizer = tfa.optimizers.AdamW(\n",
        "        learning_rate=learning_rate, weight_decay=weight_decay\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[\n",
        "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
        "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    checkpoint_filepath = \"/tmp/checkpoint\"\n",
        "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "        checkpoint_filepath,\n",
        "        monitor=\"val_accuracy\",\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "        x=x_train,\n",
        "        y=y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=num_epochs,\n",
        "        validation_split=0.1,\n",
        "        callbacks=[checkpoint_callback],\n",
        "    )\n",
        "\n",
        "    model.load_weights(checkpoint_filepath)\n",
        "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
        "\n",
        "    return history\n",
        "\n",
        "\n",
        "history = run_experiment(model)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "176/176 [==============================] - 45s 69ms/step - loss: 6.3206 - accuracy: 0.0390 - top-5-accuracy: 0.1337 - val_loss: 5.9266 - val_accuracy: 0.0100 - val_top-5-accuracy: 0.0446\n",
            "Epoch 2/100\n",
            "176/176 [==============================] - 11s 63ms/step - loss: 4.3536 - accuracy: 0.1011 - top-5-accuracy: 0.2861 - val_loss: 4.3153 - val_accuracy: 0.0738 - val_top-5-accuracy: 0.2172\n",
            "Epoch 3/100\n",
            "176/176 [==============================] - 11s 65ms/step - loss: 3.7865 - accuracy: 0.1541 - top-5-accuracy: 0.3919 - val_loss: 3.5964 - val_accuracy: 0.1516 - val_top-5-accuracy: 0.3966\n",
            "Epoch 4/100\n",
            "176/176 [==============================] - 12s 66ms/step - loss: 3.4569 - accuracy: 0.1987 - top-5-accuracy: 0.4587 - val_loss: 3.2613 - val_accuracy: 0.2312 - val_top-5-accuracy: 0.5020\n",
            "Epoch 5/100\n",
            "176/176 [==============================] - 11s 65ms/step - loss: 3.2048 - accuracy: 0.2358 - top-5-accuracy: 0.5158 - val_loss: 2.9675 - val_accuracy: 0.2782 - val_top-5-accuracy: 0.5642\n",
            "Epoch 6/100\n",
            "176/176 [==============================] - 11s 65ms/step - loss: 3.0361 - accuracy: 0.2618 - top-5-accuracy: 0.5527 - val_loss: 2.8475 - val_accuracy: 0.3028 - val_top-5-accuracy: 0.5914\n",
            "Epoch 7/100\n",
            "176/176 [==============================] - 11s 64ms/step - loss: 2.8750 - accuracy: 0.2928 - top-5-accuracy: 0.5885 - val_loss: 2.8747 - val_accuracy: 0.2942 - val_top-5-accuracy: 0.5868\n",
            "Epoch 8/100\n",
            "176/176 [==============================] - 11s 64ms/step - loss: 2.7643 - accuracy: 0.3146 - top-5-accuracy: 0.6130 - val_loss: 2.6756 - val_accuracy: 0.3416 - val_top-5-accuracy: 0.6350\n",
            "Epoch 9/100\n",
            "176/176 [==============================] - 11s 64ms/step - loss: 2.6653 - accuracy: 0.3306 - top-5-accuracy: 0.6389 - val_loss: 2.6376 - val_accuracy: 0.3444 - val_top-5-accuracy: 0.6450\n",
            "Epoch 10/100\n",
            "176/176 [==============================] - 11s 64ms/step - loss: 2.5665 - accuracy: 0.3546 - top-5-accuracy: 0.6583 - val_loss: 2.6553 - val_accuracy: 0.3434 - val_top-5-accuracy: 0.6430\n",
            "Epoch 11/100\n",
            "176/176 [==============================] - 11s 64ms/step - loss: 2.4838 - accuracy: 0.3702 - top-5-accuracy: 0.6775 - val_loss: 2.5009 - val_accuracy: 0.3652 - val_top-5-accuracy: 0.6712\n",
            "Epoch 12/100\n",
            "176/176 [==============================] - 11s 65ms/step - loss: 2.4096 - accuracy: 0.3834 - top-5-accuracy: 0.6925 - val_loss: 2.3816 - val_accuracy: 0.3914 - val_top-5-accuracy: 0.6996\n",
            "Epoch 13/100\n",
            "176/176 [==============================] - 11s 65ms/step - loss: 2.3392 - accuracy: 0.3992 - top-5-accuracy: 0.7057 - val_loss: 2.3838 - val_accuracy: 0.4000 - val_top-5-accuracy: 0.6980\n",
            "Epoch 14/100\n",
            "176/176 [==============================] - 11s 64ms/step - loss: 2.2697 - accuracy: 0.4101 - top-5-accuracy: 0.7170 - val_loss: 2.3943 - val_accuracy: 0.3870 - val_top-5-accuracy: 0.6968\n",
            "Epoch 15/100\n",
            "176/176 [==============================] - 11s 64ms/step - loss: 2.2161 - accuracy: 0.4228 - top-5-accuracy: 0.7310 - val_loss: 2.4549 - val_accuracy: 0.3862 - val_top-5-accuracy: 0.6938\n",
            "Epoch 16/100\n",
            "176/176 [==============================] - 11s 64ms/step - loss: 2.1610 - accuracy: 0.4361 - top-5-accuracy: 0.7427 - val_loss: 2.2809 - val_accuracy: 0.4156 - val_top-5-accuracy: 0.7186\n",
            "Epoch 17/100\n",
            "176/176 [==============================] - 11s 64ms/step - loss: 2.1081 - accuracy: 0.4460 - top-5-accuracy: 0.7535 - val_loss: 2.1971 - val_accuracy: 0.4346 - val_top-5-accuracy: 0.7366\n",
            "Epoch 18/100\n",
            "176/176 [==============================] - 11s 65ms/step - loss: 2.0789 - accuracy: 0.4524 - top-5-accuracy: 0.7586 - val_loss: 2.1648 - val_accuracy: 0.4424 - val_top-5-accuracy: 0.7492\n",
            "Epoch 19/100\n",
            "176/176 [==============================] - 11s 65ms/step - loss: 2.0316 - accuracy: 0.4636 - top-5-accuracy: 0.7671 - val_loss: 2.2270 - val_accuracy: 0.4288 - val_top-5-accuracy: 0.7322\n",
            "Epoch 20/100\n",
            "176/176 [==============================] - 11s 64ms/step - loss: 1.9801 - accuracy: 0.4737 - top-5-accuracy: 0.7773 - val_loss: 2.1058 - val_accuracy: 0.4514 - val_top-5-accuracy: 0.7522\n",
            "Epoch 21/100\n",
            "176/176 [==============================] - 11s 64ms/step - loss: 1.9427 - accuracy: 0.4814 - top-5-accuracy: 0.7817 - val_loss: 2.0937 - val_accuracy: 0.4552 - val_top-5-accuracy: 0.7610\n",
            "Epoch 22/100\n",
            "176/176 [==============================] - 11s 64ms/step - loss: 1.8975 - accuracy: 0.4934 - top-5-accuracy: 0.7921 - val_loss: 2.0593 - val_accuracy: 0.4638 - val_top-5-accuracy: 0.7680\n",
            "Epoch 23/100\n",
            "176/176 [==============================] - 11s 65ms/step - loss: 1.8749 - accuracy: 0.4974 - top-5-accuracy: 0.7978 - val_loss: 2.0075 - val_accuracy: 0.4774 - val_top-5-accuracy: 0.7750\n",
            "Epoch 24/100\n",
            "176/176 [==============================] - 11s 65ms/step - loss: 1.8387 - accuracy: 0.5072 - top-5-accuracy: 0.8014 - val_loss: 2.2068 - val_accuracy: 0.4408 - val_top-5-accuracy: 0.7364\n",
            "Epoch 25/100\n",
            "176/176 [==============================] - 11s 65ms/step - loss: 1.8139 - accuracy: 0.5104 - top-5-accuracy: 0.8092 - val_loss: 1.9908 - val_accuracy: 0.4820 - val_top-5-accuracy: 0.7770\n",
            "Epoch 26/100\n",
            "176/176 [==============================] - 11s 64ms/step - loss: 1.7741 - accuracy: 0.5163 - top-5-accuracy: 0.8156 - val_loss: 2.1120 - val_accuracy: 0.4524 - val_top-5-accuracy: 0.7548\n",
            "Epoch 27/100\n",
            "176/176 [==============================] - 11s 63ms/step - loss: 1.7624 - accuracy: 0.5230 - top-5-accuracy: 0.8160 - val_loss: 2.0733 - val_accuracy: 0.4642 - val_top-5-accuracy: 0.7670\n",
            "Epoch 28/100\n",
            "176/176 [==============================] - 11s 63ms/step - loss: 1.7182 - accuracy: 0.5310 - top-5-accuracy: 0.8244 - val_loss: 1.9848 - val_accuracy: 0.4732 - val_top-5-accuracy: 0.7748\n",
            "Epoch 29/100\n",
            "176/176 [==============================] - 11s 64ms/step - loss: 1.7003 - accuracy: 0.5383 - top-5-accuracy: 0.8268 - val_loss: 1.9344 - val_accuracy: 0.4928 - val_top-5-accuracy: 0.7872\n",
            "Epoch 30/100\n",
            "176/176 [==============================] - 11s 64ms/step - loss: 1.6768 - accuracy: 0.5415 - top-5-accuracy: 0.8309 - val_loss: 1.9070 - val_accuracy: 0.4896 - val_top-5-accuracy: 0.7890\n",
            "Epoch 31/100\n",
            "176/176 [==============================] - 11s 64ms/step - loss: 1.6489 - accuracy: 0.5478 - top-5-accuracy: 0.8366 - val_loss: 1.8651 - val_accuracy: 0.5060 - val_top-5-accuracy: 0.7980\n",
            "Epoch 32/100\n",
            "176/176 [==============================] - 11s 64ms/step - loss: 1.6262 - accuracy: 0.5525 - top-5-accuracy: 0.8372 - val_loss: 1.8793 - val_accuracy: 0.5100 - val_top-5-accuracy: 0.7962\n",
            "Epoch 33/100\n",
            "176/176 [==============================] - 11s 64ms/step - loss: 1.6107 - accuracy: 0.5553 - top-5-accuracy: 0.8434 - val_loss: 1.8420 - val_accuracy: 0.5024 - val_top-5-accuracy: 0.8032\n",
            "Epoch 34/100\n",
            "176/176 [==============================] - 11s 64ms/step - loss: 1.5966 - accuracy: 0.5625 - top-5-accuracy: 0.8453 - val_loss: 1.9082 - val_accuracy: 0.4978 - val_top-5-accuracy: 0.7954\n",
            "Epoch 35/100\n",
            "176/176 [==============================] - 11s 64ms/step - loss: 1.5679 - accuracy: 0.5657 - top-5-accuracy: 0.8493 - val_loss: 1.8119 - val_accuracy: 0.5182 - val_top-5-accuracy: 0.8022\n",
            "Epoch 36/100\n",
            "176/176 [==============================] - 11s 63ms/step - loss: 1.5532 - accuracy: 0.5685 - top-5-accuracy: 0.8516 - val_loss: 1.8431 - val_accuracy: 0.5156 - val_top-5-accuracy: 0.8064\n",
            "Epoch 37/100\n",
            "176/176 [==============================] - 11s 64ms/step - loss: 1.5323 - accuracy: 0.5722 - top-5-accuracy: 0.8570 - val_loss: 1.7734 - val_accuracy: 0.5226 - val_top-5-accuracy: 0.8146\n",
            "Epoch 38/100\n",
            "176/176 [==============================] - 11s 65ms/step - loss: 1.5143 - accuracy: 0.5768 - top-5-accuracy: 0.8588 - val_loss: 1.8543 - val_accuracy: 0.5062 - val_top-5-accuracy: 0.8050\n",
            "Epoch 39/100\n",
            "176/176 [==============================] - 11s 64ms/step - loss: 1.5051 - accuracy: 0.5793 - top-5-accuracy: 0.8590 - val_loss: 1.9059 - val_accuracy: 0.4974 - val_top-5-accuracy: 0.7940\n",
            "Epoch 40/100\n",
            "176/176 [==============================] - 11s 64ms/step - loss: 1.4858 - accuracy: 0.5838 - top-5-accuracy: 0.8630 - val_loss: 1.8595 - val_accuracy: 0.5050 - val_top-5-accuracy: 0.8098\n",
            "Epoch 41/100\n",
            "176/176 [==============================] - 11s 64ms/step - loss: 1.4676 - accuracy: 0.5889 - top-5-accuracy: 0.8654 - val_loss: 1.7850 - val_accuracy: 0.5186 - val_top-5-accuracy: 0.8132\n",
            "Epoch 42/100\n",
            "176/176 [==============================] - 11s 64ms/step - loss: 1.4514 - accuracy: 0.5920 - top-5-accuracy: 0.8690 - val_loss: 1.7346 - val_accuracy: 0.5370 - val_top-5-accuracy: 0.8174\n",
            "Epoch 43/100\n",
            "176/176 [==============================] - 11s 64ms/step - loss: 1.4419 - accuracy: 0.5940 - top-5-accuracy: 0.8698 - val_loss: 1.7905 - val_accuracy: 0.5328 - val_top-5-accuracy: 0.8158\n",
            "Epoch 44/100\n",
            "176/176 [==============================] - 11s 65ms/step - loss: 1.4177 - accuracy: 0.6022 - top-5-accuracy: 0.8728 - val_loss: 1.6914 - val_accuracy: 0.5446 - val_top-5-accuracy: 0.8248\n",
            "Epoch 45/100\n",
            "176/176 [==============================] - 11s 65ms/step - loss: 1.4177 - accuracy: 0.6001 - top-5-accuracy: 0.8744 - val_loss: 1.7137 - val_accuracy: 0.5438 - val_top-5-accuracy: 0.8220\n",
            "Epoch 46/100\n",
            "176/176 [==============================] - 11s 65ms/step - loss: 1.4029 - accuracy: 0.6025 - top-5-accuracy: 0.8779 - val_loss: 1.7283 - val_accuracy: 0.5402 - val_top-5-accuracy: 0.8244\n",
            "Epoch 47/100\n",
            "176/176 [==============================] - 11s 64ms/step - loss: 1.3943 - accuracy: 0.6063 - top-5-accuracy: 0.8775 - val_loss: 1.7779 - val_accuracy: 0.5330 - val_top-5-accuracy: 0.8184\n",
            "Epoch 48/100\n",
            "176/176 [==============================] - 11s 64ms/step - loss: 1.3763 - accuracy: 0.6079 - top-5-accuracy: 0.8810 - val_loss: 1.6930 - val_accuracy: 0.5430 - val_top-5-accuracy: 0.8230\n",
            "Epoch 49/100\n",
            "176/176 [==============================] - 11s 65ms/step - loss: 1.3619 - accuracy: 0.6141 - top-5-accuracy: 0.8813 - val_loss: 1.6945 - val_accuracy: 0.5396 - val_top-5-accuracy: 0.8274\n",
            "Epoch 50/100\n",
            "176/176 [==============================] - 11s 65ms/step - loss: 1.3534 - accuracy: 0.6159 - top-5-accuracy: 0.8848 - val_loss: 1.7567 - val_accuracy: 0.5318 - val_top-5-accuracy: 0.8140\n",
            "Epoch 51/100\n",
            "176/176 [==============================] - 11s 65ms/step - loss: 1.3392 - accuracy: 0.6205 - top-5-accuracy: 0.8856 - val_loss: 1.8895 - val_accuracy: 0.5202 - val_top-5-accuracy: 0.7960\n",
            "Epoch 52/100\n",
            "176/176 [==============================] - 11s 65ms/step - loss: 1.3355 - accuracy: 0.6194 - top-5-accuracy: 0.8849 - val_loss: 1.8026 - val_accuracy: 0.5378 - val_top-5-accuracy: 0.8122\n",
            "Epoch 53/100\n",
            "176/176 [==============================] - 11s 64ms/step - loss: 1.3167 - accuracy: 0.6252 - top-5-accuracy: 0.8890 - val_loss: 1.8233 - val_accuracy: 0.5246 - val_top-5-accuracy: 0.7982\n",
            "Epoch 54/100\n",
            "176/176 [==============================] - 11s 64ms/step - loss: 1.3063 - accuracy: 0.6270 - top-5-accuracy: 0.8926 - val_loss: 1.6837 - val_accuracy: 0.5470 - val_top-5-accuracy: 0.8240\n",
            "Epoch 55/100\n",
            "176/176 [==============================] - 11s 65ms/step - loss: 1.2970 - accuracy: 0.6269 - top-5-accuracy: 0.8904 - val_loss: 1.6529 - val_accuracy: 0.5562 - val_top-5-accuracy: 0.8316\n",
            "Epoch 56/100\n",
            "176/176 [==============================] - 12s 65ms/step - loss: 1.2964 - accuracy: 0.6289 - top-5-accuracy: 0.8931 - val_loss: 1.7207 - val_accuracy: 0.5442 - val_top-5-accuracy: 0.8182\n",
            "Epoch 57/100\n",
            "176/176 [==============================] - 12s 66ms/step - loss: 1.2790 - accuracy: 0.6350 - top-5-accuracy: 0.8961 - val_loss: 1.6320 - val_accuracy: 0.5642 - val_top-5-accuracy: 0.8366\n",
            "Epoch 58/100\n",
            "176/176 [==============================] - 12s 65ms/step - loss: 1.2741 - accuracy: 0.6369 - top-5-accuracy: 0.8944 - val_loss: 1.6620 - val_accuracy: 0.5506 - val_top-5-accuracy: 0.8314\n",
            "Epoch 59/100\n",
            "176/176 [==============================] - 11s 65ms/step - loss: 1.2719 - accuracy: 0.6366 - top-5-accuracy: 0.8971 - val_loss: 1.8882 - val_accuracy: 0.5124 - val_top-5-accuracy: 0.7976\n",
            "Epoch 60/100\n",
            "176/176 [==============================] - 11s 65ms/step - loss: 1.2537 - accuracy: 0.6388 - top-5-accuracy: 0.8992 - val_loss: 1.6890 - val_accuracy: 0.5434 - val_top-5-accuracy: 0.8262\n",
            "Epoch 61/100\n",
            "176/176 [==============================] - 12s 66ms/step - loss: 1.2417 - accuracy: 0.6432 - top-5-accuracy: 0.9006 - val_loss: 1.7185 - val_accuracy: 0.5468 - val_top-5-accuracy: 0.8252\n",
            "Epoch 62/100\n",
            "176/176 [==============================] - 12s 66ms/step - loss: 1.2431 - accuracy: 0.6405 - top-5-accuracy: 0.8999 - val_loss: 1.6601 - val_accuracy: 0.5470 - val_top-5-accuracy: 0.8332\n",
            "Epoch 63/100\n",
            "176/176 [==============================] - 12s 66ms/step - loss: 1.2388 - accuracy: 0.6418 - top-5-accuracy: 0.9013 - val_loss: 1.6575 - val_accuracy: 0.5564 - val_top-5-accuracy: 0.8354\n",
            "Epoch 64/100\n",
            "176/176 [==============================] - 12s 66ms/step - loss: 1.2189 - accuracy: 0.6471 - top-5-accuracy: 0.9034 - val_loss: 1.5730 - val_accuracy: 0.5742 - val_top-5-accuracy: 0.8386\n",
            "Epoch 65/100\n",
            "176/176 [==============================] - 12s 66ms/step - loss: 1.2118 - accuracy: 0.6505 - top-5-accuracy: 0.9048 - val_loss: 1.6718 - val_accuracy: 0.5500 - val_top-5-accuracy: 0.8352\n",
            "Epoch 66/100\n",
            "176/176 [==============================] - 12s 66ms/step - loss: 1.2103 - accuracy: 0.6501 - top-5-accuracy: 0.9043 - val_loss: 1.6724 - val_accuracy: 0.5488 - val_top-5-accuracy: 0.8344\n",
            "Epoch 67/100\n",
            "176/176 [==============================] - 11s 65ms/step - loss: 1.2059 - accuracy: 0.6515 - top-5-accuracy: 0.9058 - val_loss: 1.6526 - val_accuracy: 0.5540 - val_top-5-accuracy: 0.8324\n",
            "Epoch 68/100\n",
            "176/176 [==============================] - 11s 65ms/step - loss: 1.1983 - accuracy: 0.6517 - top-5-accuracy: 0.9070 - val_loss: 1.8205 - val_accuracy: 0.5230 - val_top-5-accuracy: 0.8128\n",
            "Epoch 69/100\n",
            "176/176 [==============================] - 12s 66ms/step - loss: 1.1869 - accuracy: 0.6551 - top-5-accuracy: 0.9080 - val_loss: 1.6301 - val_accuracy: 0.5584 - val_top-5-accuracy: 0.8318\n",
            "Epoch 70/100\n",
            "176/176 [==============================] - 12s 66ms/step - loss: 1.1873 - accuracy: 0.6588 - top-5-accuracy: 0.9075 - val_loss: 1.5874 - val_accuracy: 0.5726 - val_top-5-accuracy: 0.8454\n",
            "Epoch 71/100\n",
            "176/176 [==============================] - 12s 66ms/step - loss: 1.1866 - accuracy: 0.6562 - top-5-accuracy: 0.9079 - val_loss: 1.7407 - val_accuracy: 0.5398 - val_top-5-accuracy: 0.8222\n",
            "Epoch 72/100\n",
            "176/176 [==============================] - 12s 65ms/step - loss: 1.1665 - accuracy: 0.6603 - top-5-accuracy: 0.9117 - val_loss: 1.6755 - val_accuracy: 0.5440 - val_top-5-accuracy: 0.8324\n",
            "Epoch 73/100\n",
            "176/176 [==============================] - 12s 65ms/step - loss: 1.1546 - accuracy: 0.6631 - top-5-accuracy: 0.9115 - val_loss: 1.6534 - val_accuracy: 0.5558 - val_top-5-accuracy: 0.8312\n",
            "Epoch 74/100\n",
            "176/176 [==============================] - 12s 65ms/step - loss: 1.1703 - accuracy: 0.6600 - top-5-accuracy: 0.9115 - val_loss: 1.6698 - val_accuracy: 0.5624 - val_top-5-accuracy: 0.8300\n",
            "Epoch 75/100\n",
            "176/176 [==============================] - 12s 66ms/step - loss: 1.1635 - accuracy: 0.6634 - top-5-accuracy: 0.9118 - val_loss: 1.6008 - val_accuracy: 0.5650 - val_top-5-accuracy: 0.8392\n",
            "Epoch 76/100\n",
            "176/176 [==============================] - 12s 66ms/step - loss: 1.1449 - accuracy: 0.6669 - top-5-accuracy: 0.9142 - val_loss: 1.6607 - val_accuracy: 0.5536 - val_top-5-accuracy: 0.8384\n",
            "Epoch 77/100\n",
            "176/176 [==============================] - 12s 66ms/step - loss: 1.1329 - accuracy: 0.6683 - top-5-accuracy: 0.9166 - val_loss: 1.6927 - val_accuracy: 0.5514 - val_top-5-accuracy: 0.8250\n",
            "Epoch 78/100\n",
            "176/176 [==============================] - 12s 66ms/step - loss: 1.1259 - accuracy: 0.6692 - top-5-accuracy: 0.9170 - val_loss: 1.7607 - val_accuracy: 0.5472 - val_top-5-accuracy: 0.8296\n",
            "Epoch 79/100\n",
            "176/176 [==============================] - 12s 66ms/step - loss: 1.1389 - accuracy: 0.6644 - top-5-accuracy: 0.9133 - val_loss: 1.6498 - val_accuracy: 0.5660 - val_top-5-accuracy: 0.8356\n",
            "Epoch 80/100\n",
            "176/176 [==============================] - 11s 65ms/step - loss: 1.1228 - accuracy: 0.6724 - top-5-accuracy: 0.9161 - val_loss: 1.6425 - val_accuracy: 0.5586 - val_top-5-accuracy: 0.8404\n",
            "Epoch 81/100\n",
            "176/176 [==============================] - 12s 66ms/step - loss: 1.1240 - accuracy: 0.6720 - top-5-accuracy: 0.9160 - val_loss: 1.6569 - val_accuracy: 0.5636 - val_top-5-accuracy: 0.8356\n",
            "Epoch 82/100\n",
            "176/176 [==============================] - 12s 66ms/step - loss: 1.1202 - accuracy: 0.6707 - top-5-accuracy: 0.9173 - val_loss: 1.6403 - val_accuracy: 0.5560 - val_top-5-accuracy: 0.8398\n",
            "Epoch 83/100\n",
            "176/176 [==============================] - 12s 66ms/step - loss: 1.1214 - accuracy: 0.6710 - top-5-accuracy: 0.9179 - val_loss: 1.6312 - val_accuracy: 0.5606 - val_top-5-accuracy: 0.8374\n",
            "Epoch 84/100\n",
            "176/176 [==============================] - 12s 66ms/step - loss: 1.1217 - accuracy: 0.6691 - top-5-accuracy: 0.9179 - val_loss: 1.6756 - val_accuracy: 0.5528 - val_top-5-accuracy: 0.8282\n",
            "Epoch 85/100\n",
            "176/176 [==============================] - 11s 65ms/step - loss: 1.1038 - accuracy: 0.6760 - top-5-accuracy: 0.9203 - val_loss: 1.5956 - val_accuracy: 0.5658 - val_top-5-accuracy: 0.8416\n",
            "Epoch 86/100\n",
            "176/176 [==============================] - 12s 65ms/step - loss: 1.1024 - accuracy: 0.6757 - top-5-accuracy: 0.9212 - val_loss: 1.7250 - val_accuracy: 0.5572 - val_top-5-accuracy: 0.8374\n",
            "Epoch 87/100\n",
            "176/176 [==============================] - 12s 66ms/step - loss: 1.0983 - accuracy: 0.6772 - top-5-accuracy: 0.9202 - val_loss: 1.6476 - val_accuracy: 0.5656 - val_top-5-accuracy: 0.8382\n",
            "Epoch 88/100\n",
            "176/176 [==============================] - 12s 66ms/step - loss: 1.0913 - accuracy: 0.6784 - top-5-accuracy: 0.9221 - val_loss: 1.6430 - val_accuracy: 0.5604 - val_top-5-accuracy: 0.8422\n",
            "Epoch 89/100\n",
            "176/176 [==============================] - 12s 66ms/step - loss: 1.0863 - accuracy: 0.6792 - top-5-accuracy: 0.9239 - val_loss: 1.7245 - val_accuracy: 0.5438 - val_top-5-accuracy: 0.8242\n",
            "Epoch 90/100\n",
            "176/176 [==============================] - 12s 66ms/step - loss: 1.0825 - accuracy: 0.6790 - top-5-accuracy: 0.9218 - val_loss: 1.7759 - val_accuracy: 0.5522 - val_top-5-accuracy: 0.8252\n",
            "Epoch 91/100\n",
            "176/176 [==============================] - 12s 66ms/step - loss: 1.0868 - accuracy: 0.6806 - top-5-accuracy: 0.9215 - val_loss: 1.5599 - val_accuracy: 0.5826 - val_top-5-accuracy: 0.8448\n",
            "Epoch 92/100\n",
            "176/176 [==============================] - 12s 65ms/step - loss: 1.0793 - accuracy: 0.6795 - top-5-accuracy: 0.9233 - val_loss: 1.5856 - val_accuracy: 0.5772 - val_top-5-accuracy: 0.8426\n",
            "Epoch 93/100\n",
            "176/176 [==============================] - 12s 65ms/step - loss: 1.0630 - accuracy: 0.6880 - top-5-accuracy: 0.9244 - val_loss: 1.5900 - val_accuracy: 0.5734 - val_top-5-accuracy: 0.8486\n",
            "Epoch 94/100\n",
            "176/176 [==============================] - 12s 66ms/step - loss: 1.0637 - accuracy: 0.6852 - top-5-accuracy: 0.9238 - val_loss: 1.5807 - val_accuracy: 0.5692 - val_top-5-accuracy: 0.8500\n",
            "Epoch 95/100\n",
            "176/176 [==============================] - 12s 66ms/step - loss: 1.0692 - accuracy: 0.6837 - top-5-accuracy: 0.9245 - val_loss: 1.6509 - val_accuracy: 0.5694 - val_top-5-accuracy: 0.8354\n",
            "Epoch 96/100\n",
            "176/176 [==============================] - 12s 66ms/step - loss: 1.0629 - accuracy: 0.6860 - top-5-accuracy: 0.9256 - val_loss: 1.7293 - val_accuracy: 0.5492 - val_top-5-accuracy: 0.8238\n",
            "Epoch 97/100\n",
            "176/176 [==============================] - 12s 66ms/step - loss: 1.0622 - accuracy: 0.6867 - top-5-accuracy: 0.9259 - val_loss: 1.5383 - val_accuracy: 0.5790 - val_top-5-accuracy: 0.8514\n",
            "Epoch 98/100\n",
            "176/176 [==============================] - 12s 66ms/step - loss: 1.0592 - accuracy: 0.6878 - top-5-accuracy: 0.9254 - val_loss: 1.6487 - val_accuracy: 0.5674 - val_top-5-accuracy: 0.8342\n",
            "Epoch 99/100\n",
            "176/176 [==============================] - 12s 66ms/step - loss: 1.0551 - accuracy: 0.6883 - top-5-accuracy: 0.9267 - val_loss: 1.5491 - val_accuracy: 0.5790 - val_top-5-accuracy: 0.8516\n",
            "Epoch 100/100\n",
            "176/176 [==============================] - 12s 66ms/step - loss: 1.0532 - accuracy: 0.6895 - top-5-accuracy: 0.9269 - val_loss: 1.6115 - val_accuracy: 0.5802 - val_top-5-accuracy: 0.8466\n",
            "313/313 [==============================] - 2s 5ms/step - loss: 1.5194 - accuracy: 0.5955 - top-5-accuracy: 0.8510\n",
            "Test accuracy: 59.55%\n",
            "Test top 5 accuracy: 85.1%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
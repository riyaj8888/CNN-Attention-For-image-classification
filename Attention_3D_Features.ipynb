{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Attention_3D_Features.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXZfE5HALCoK"
      },
      "source": [
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class key_embedd(nn.Module):\n",
        "\n",
        "  def __init__(self, in_channels, out_channels, kernel_size, stride=(1,2,2), padding=(1,1,1), bias=True):\n",
        "\n",
        "    super(key_embedd,self).__init__()\n",
        "\n",
        "    self.key_dim = out_channels\n",
        "\n",
        "    self.spatial_conv = nn.Conv3d(in_channels, self.key_dim, kernel_size,stride=(1,1,1), padding=padding,bias=bias)\n",
        "\n",
        "    self.bn = nn.BatchNorm3d(self.key_dim)\n",
        "\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "    self.spatial_conv1 = nn.Conv3d(self.key_dim, self.key_dim+32, kernel_size,stride=stride, padding=padding,bias=bias)\n",
        "\n",
        "    self.bn1 = nn.BatchNorm3d(self.key_dim+32)\n",
        "\n",
        "    self.relu1 = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    # X = (B,W,N,H,W,C)\n",
        "\n",
        "    # x = x.view(x.shape[0],x.shape[1],x.shape[2]*x.shape[3],x.shape[4],x.shape[5])\n",
        "    # print(x.shape)\n",
        "\n",
        "    x = self.relu(self.bn(self.spatial_conv(x)))\n",
        "\n",
        "    x = self.relu1(self.bn1(self.spatial_conv1(x)))\n",
        "\n",
        "    shape = x.shape\n",
        "    x = x.view(x.shape[0],x.shape[1],-1)\n",
        "    return x,shape"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8yGpwIVLOpd"
      },
      "source": [
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class value_embedd(nn.Module):\n",
        "\n",
        "  def __init__(self, in_channels, out_channels, kernel_size, stride=(1,2,2), padding=(1,1,1), bias=True):\n",
        "\n",
        "    super(value_embedd,self).__init__()\n",
        "\n",
        "    self.key_dim = out_channels\n",
        "\n",
        "    self.spatial_conv = nn.Conv3d(in_channels, self.key_dim, kernel_size,stride=(1,1,1), padding=padding,bias=bias)\n",
        "\n",
        "    self.bn = nn.BatchNorm3d(self.key_dim)\n",
        "\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "    self.spatial_conv1 = nn.Conv3d(self.key_dim, self.key_dim+32, kernel_size,stride=stride, padding=padding,bias=bias)\n",
        "\n",
        "    self.bn1 = nn.BatchNorm3d(self.key_dim+32)\n",
        "\n",
        "    self.relu1 = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    # x = x.view(x.shape[0],x.shape[1],x.shape[2]*x.shape[3],x.shape[4],x.shape[5])\n",
        "    # print(x.shape)\n",
        "\n",
        "\n",
        "    x = self.relu(self.bn(self.spatial_conv(x)))\n",
        "\n",
        "    x = self.relu1(self.bn1(self.spatial_conv1(x)))\n",
        "\n",
        "    \n",
        "    shape = x.shape\n",
        "    x = x.view(x.shape[0],x.shape[1],-1)\n",
        "    return x,shape"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-XfoUFbLYtR"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class query_embedd(nn.Module):\n",
        "\n",
        "  def __init__(self, in_channels, out_channels, kernel_size, stride=(1,2,2), padding=(1,1,1), bias=True):\n",
        "\n",
        "    super(query_embedd,self).__init__()\n",
        "\n",
        "    self.key_dim = out_channels\n",
        "\n",
        "    self.spatial_conv = nn.Conv3d(in_channels, self.key_dim, kernel_size,stride=(1,1,1), padding=padding,bias=bias)\n",
        "\n",
        "    self.bn = nn.BatchNorm3d(self.key_dim)\n",
        "\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "    self.spatial_conv1 = nn.Conv3d(self.key_dim, self.key_dim+32, kernel_size,stride=stride, padding=padding,bias=bias)\n",
        "\n",
        "    self.bn1 = nn.BatchNorm3d(self.key_dim+32)\n",
        "\n",
        "    self.relu1 = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    # x = x.view(x.shape[0],x.shape[1],x.shape[2]*x.shape[3],x.shape[4],x.shape[5])\n",
        "\n",
        "\n",
        "    x = self.relu(self.bn(self.spatial_conv(x)))\n",
        "\n",
        "    x = self.relu1(self.bn1(self.spatial_conv1(x)))\n",
        "\n",
        "    shape = x.shape\n",
        "    \n",
        "    x = x.view(x.shape[0],x.shape[1],-1)\n",
        "\n",
        "    # print(x.shape,shape)\n",
        "\n",
        "    return x,shape"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OFTo0ENLUfR"
      },
      "source": [
        "#x =  torch.ones((16,12,3,8,64,64))\n",
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class key_embedd(nn.Module):\n",
        "\n",
        "  def __init__(self, in_channels, out_channels, kernel_size, stride=(1,2,2), padding=(1,1,1), bias=True):\n",
        "\n",
        "    super(key_embedd,self).__init__()\n",
        "\n",
        "    self.key_dim = out_channels\n",
        "\n",
        "    self.spatial_conv = nn.Conv3d(in_channels, self.key_dim, kernel_size,stride=(1,1,1), padding=padding,bias=bias)\n",
        "\n",
        "    self.bn = nn.BatchNorm3d(self.key_dim)\n",
        "\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "    self.spatial_conv1 = nn.Conv3d(self.key_dim, self.key_dim+32, kernel_size,stride=stride, padding=padding,bias=bias)\n",
        "\n",
        "    self.bn1 = nn.BatchNorm3d(self.key_dim+32)\n",
        "\n",
        "    self.relu1 = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    # X = (B,W,N,H,W,C)\n",
        "\n",
        "    # x = x.view(x.shape[0],x.shape[1],x.shape[2]*x.shape[3],x.shape[4],x.shape[5])\n",
        "    # print(x.shape)\n",
        "\n",
        "    x = self.relu(self.bn(self.spatial_conv(x)))\n",
        "\n",
        "    x = self.relu1(self.bn1(self.spatial_conv1(x)))\n",
        "\n",
        "    shape = x.shape\n",
        "    x = x.view(x.shape[0],x.shape[1],-1)\n",
        "    return x,shape\n",
        "\n",
        "class Attention_head(nn.Module):\n",
        "\n",
        "  def __init__(self,in_channels, out_channels, kernel_size, stride=(1,2,2), padding=(1,1,1), bias=True):\n",
        "    super(Attention_head,self).__init__()\n",
        "\n",
        "    self.key_embed = key_embedd(in_channels,out_channels,kernel_size)\n",
        "    self.query_embed = key_embedd(in_channels,out_channels,kernel_size)\n",
        "    self.value_embed = key_embedd(in_channels,out_channels//2,kernel_size)\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "    k,_ = self.key_embed(x)\n",
        "\n",
        "    v,shape1 = self.value_embed(x)\n",
        "\n",
        "    q,_ = self.query_embed(x)\n",
        "\n",
        "    score = torch.matmul(k.permute(0,2,1),q)\n",
        "\n",
        "    s = nn.Softmax(dim=2)(score)\n",
        "\n",
        "    # print(s.shape,v.shape)\n",
        "    v = torch.matmul(s,v.permute(0,2,1))\n",
        "\n",
        "    v = v.permute(0,2,1)\n",
        "\n",
        "    # print(v.shape,shape1)\n",
        "\n",
        "    v = v.view(shape1[0],shape1[1],shape1[2],shape1[3],shape1[4])\n",
        "\n",
        "    return v\n",
        "\n",
        "\n",
        "class MHAttention(nn.Module):\n",
        "\n",
        "  def __init__(self,in_channels, out_channels, kernel_size, stride=(1,2,2), padding=(1,1,1), bias=True):\n",
        "\n",
        "    super(MHAttention,self).__init__()\n",
        "    \n",
        "    self.attention1 = Attention_head(3, 32, 3, stride=(1,2,2), padding=(1,1,1), bias=True)\n",
        "    self.attention2 = Attention_head(32//2 + 32 , 64, 3, stride=(1,2,2), padding=(1,1,1), bias=True)\n",
        "    self.attention3 = Attention_head(64//2 + 32  , 128, 3, stride=(1,2,2), padding=(1,1,1), bias=True)\n",
        "    self.attention4 = Attention_head(128//2 + 32  , 256, 3, stride=(1,2,2), padding=(1,1,1), bias=True)\n",
        "\n",
        "    # self.attention = [Attention_head(in_channels, out_channels, kernel_size, stride=(1,2,2), padding=(1,1,1), bias=True)]\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "    x = self.attention1(x)\n",
        "    # print(x.shape,\"1\")\n",
        "    x = self.attention2(x)\n",
        "    # print(x.shape,\"2\")\n",
        "\n",
        "    x = self.attention3(x)\n",
        "    # print(x.shape,\"3\")\n",
        "\n",
        "    x = self.attention4(x)\n",
        "    # print(x.shape,\"4\")\n",
        "\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Slu5Iyu0Lcmx",
        "outputId": "841ec34c-af8e-47bf-f019-43b8e7c01df2"
      },
      "source": [
        "import torch\n",
        "(B,N,C,T,H,W) = (8,8,3,4,32,32)\n",
        "x =  torch.ones((8,8,3,4,32,32))\n",
        "x = x.view(B*N,3,4,32,32)\n",
        "f = MHAttention(3,16,3)(x)\n",
        "# f = Attention_head(3,24,3)(x)\n",
        "\n",
        "f.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64, 160, 4, 2, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    }
  ]
}